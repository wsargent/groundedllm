
services:

  # LiteLLM can be enabled in Open WebUI if you want direct access to models
  litellm:
    build: litellm/
    container_name: litellm
    ports:
      - 4000:4000
    environment:
      - LITELLM_LOG=WARN
      - LITELLM_MASTER_KEY=$LITELLM_MASTER_KEY
      - GEMINI_API_KEY=$GEMINI_API_KEY
      - OPENROUTER_API_KEY=$OPENROUTER_API_KEY
      - ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY
      - OLLAMA_BASE_URL=$OLLAMA_BASE_URL
      - LM_STUDIO_API_BASE=$LM_STUDIO_API_BASE
      - LM_STUDIO_API_KEY=$LM_STUDIO_API_KEY
      
  # This is the tool server and also provisions Letta & Open WebUI
  hayhooks:
    build: hayhooks/
    container_name: hayhooks
    ports:
      - 1416:1416
    volumes:
      - ./hayhooks/pipelines:/pipelines
    environment:
      - HAYHOOKS_SEARCH_MODEL=$HAYHOOKS_SEARCH_MODEL
      - HAYHOOKS_EXCERPT_MODEL=$HAYHOOKS_EXCERPT_MODEL
      - HAYHOOKS_PIPELINES_DIR=/pipelines
      - HAYHOOKS_DISABLE_SSL=true
      - HAYHOOKS_HOST=0.0.0.0
      - HAYHOOKS_PORT=1416
      - LOG_LEVEL=INFO
      - LOG=INFO
      - HAYSTACK_LOG_LEVEL=INFO
      - HAYHOOKS_SHOW_TRACEBACKS=true
      # Needed for search pipeline
      - TAVILY_API_KEY=$TAVILY_API_KEY
      - LINKUP_API_KEY=$LINKUP_API_KEY
      - OPENWEBUI_BASE_URL=http://open-webui:8080
      - LETTA_BASE_URL=http://letta:8283
      - OPENAI_API_BASE=http://litellm:4000
      - OPENAI_API_KEY=$LITELLM_MASTER_KEY
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:1416/status"]
      interval: 10s
      timeout: 5s
      retries: 18
      start_period: 5s

  # Letta is an agent building framework with built-in memory/vectordb support.
  # https://docs.letta.com/quickstart/docker
  letta:
    image: letta/letta:0.7.7
    container_name: letta
    ports:
      - 8283:8283
    volumes:
      # Mount the MCP configuration file (this points to Hayhooks)
      - ./letta_mcp_config.json:/root/.letta/mcp_config.json
    environment:
      # https://docs.letta.com/guides/server/providers/anthropic
      ANTHROPIC_API_KEY: $ANTHROPIC_API_KEY
      # https://docs.letta.com/guides/server/providers/google
      GEMINI_API_KEY: $GEMINI_API_KEY
      OLLAMA_BASE_URL: $OLLAMA_BASE_URL
      # Do not attempt to use Ollama, LM Studio or OpenRouter chat models with Letta.
      # Letta is *very* picky and will only reliably work with the major cloud providers.
    restart: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8283/v1/health/"]
      interval: 5s
      timeout: 5s
      retries: 18
      start_period: 1s

  # Open WebUI is the front-end UI to Letta
  open-webui:
    image: ghcr.io/open-webui/open-webui:0.6.6
    container_name: open-webui
    volumes:
     - open-webui:/app/backend/data
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    environment:
      # https://docs.openwebui.com/getting-started/env-configuration/
      - GLOBAL_LOG_LEVEL=WARNING
      # Disable admin login
      - WEBUI_AUTH=false
      # Enable the /docs endpoint for OpenAPI viewing
      #- ENV=dev
      # Prevent a langchain warning
      - USER_AGENT=openwebui
      #Â Set tags and titles explictly
      - ENABLE_TAGS_GENERATION=$ENABLE_TAGS_GENERATION
      - ENABLE_TITLE_GENERATION=$ENABLE_TITLE_GENERATION
      - TASK_MODEL=$TASK_MODEL
      - TASK_MODEL_EXTERNAL=$TASK_MODEL_EXTERNAL
      # Disable some meaningless options
      - ENABLE_EVALUATION_ARENA_MODELS=false
      - ENABLE_AUTOCOMPLETE_GENERATION=false
      - ENABLE_RETRIEVAL_QUERY_GENERATION=false
      # OpenAI selection should go to LiteLLM
      - ENABLE_OPENAI_API=$ENABLE_OPENAI_API
      - OPENAI_API_BASE_URL=http://litellm:4000
      - OPENAI_API_KEY=$LITELLM_MASTER_KEY
      # Ollama Options
      - ENABLE_OLLAMA_API=$ENABLE_OLLAMA_API
      - OLLAMA_BASE_URL=$OLLAMA_BASE_URL
      # RAG options can be transformers, ollama, or openai 
      - RAG_EMBEDDING_ENGINE=$RAG_EMBEDDING_ENGINE
      - RAG_EMBEDDING_MODEL=$RAG_EMBEDDING_MODEL
      - RAG_OPENAI_API_BASE_URL=http://litellm:4000
      - RAG_OPENAI_API_KEY=$LITELLM_MASTER_KEY
      - RAG_OLLAMA_BASE_URL=$OLLAMA_BASE_URL
      # Tavily Web Search in Open WebUI
      - ENABLE_WEB_SEARCH=$ENABLE_WEB_SEARCH
      - WEB_SEARCH_ENGINE=$WEB_SEARCH_ENGINE
      - TAVILY_API_KEY=$TAVILY_API_KEY
      # Audio options
      - AUDIO_STT_ENGINE=$AUDIO_STT_ENGINE
    restart: unless-stopped
    # https://docs.openwebui.com/getting-started/advanced-topics/monitoring/#basic-health-check-endpoint
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 18
      start_period: 5s

  # This service runs after open-webui and letta has come up and is healthy
  # and adds an agent and tools to Letta and changes settings in Open WebUI.
  initializer:
    build: ./initializer
    container_name: initializer
    depends_on:
      open-webui:
        condition: service_healthy
      letta:
        condition: service_healthy
    environment:
      - OPEN_WEBUI_URL=http://open-webui:8080
      - LETTA_BASE_URL=http://letta:8283
      - HAYHOOKS_BASE_URL=http://hayhooks:1416
      - CHAT_MODEL=$LETTA_CHAT_MODEL
      - EMBEDDING_MODEL=$LETTA_EMBEDDING_MODEL

volumes:
  open-webui:
