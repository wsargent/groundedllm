
services:

  # LiteLLM can be enabled in Open WebUI if you want direct access to models
  litellm:
    build: litellm/
    container_name: litellm
    ports:
      - 4000:4000
    env_file:
      - ".env"
    environment:
      - LITELLM_LOG=WARN
      - LITELLM_MASTER_KEY=$LITELLM_MASTER_KEY
      
  caddy:
    image: caddy:2
    container_name: caddy
    ports:
      - "443:443"
    volumes:
      - ./caddy/Caddyfile:/etc/caddy/Caddyfile
      - ./caddy/caddy+2.pem:/etc/caddy/caddy+2.pem
      - ./caddy/caddy+2-key.pem:/etc/caddy/caddy+2-key.pem
      - caddy_data:/data
      - caddy_config:/config
    command: sh -c "update-ca-certificates && caddy run --config /etc/caddy/Caddyfile"
    # See logs in real-time
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "https://127.0.0.1:443/status"]
      interval: 10s
      timeout: 5s
      retries: 18
      start_period: 5s
        
  # This is the tool server and also provisions Letta & Open WebUI
  hayhooks:
    build:
      context: .
      dockerfile: hayhooks/Dockerfile
    container_name: hayhooks
    ports:
      - 1416:1416
    volumes:
      - ./hayhooks/pipelines:/pipelines
      # https://stackoverflow.com/questions/39172652/using-docker-compose-to-set-containers-timezones
      - "/etc/localtime:/etc/localtime:ro"
      - "/etc/timezone:/etc/timezone:ro"
    env_file:
      - ".env"
    environment:
      - HAYHOOKS_PIPELINES_DIR=/pipelines
      - HAYHOOKS_DISABLE_SSL=true
      - HAYHOOKS_HOST=0.0.0.0
      - HAYHOOKS_PORT=1416
      - LOG_LEVEL=${HAYHOOKS_LOG_LEVEL:-WARNING}
      - LOG=${HAYHOOKS_LOG_LEVEL:-WARNING}
      - HAYSTACK_LOG_LEVEL=${HAYHOOKS_LOG_LEVEL:-WARNING}
      - HAYHOOKS_SHOW_TRACEBACKS=true      
      - HAYHOOKS_USER_ID=test_user
      - OAUTHLIB_INSECURE_TRANSPORT=1
      # Connections to other containers
      - OPENWEBUI_BASE_URL=http://open-webui:8080
      - LETTA_BASE_URL=http://letta:8283
      - OPENAI_API_BASE=http://litellm:4000
      - SEARXNG_BASE_URL=http://searxng:8080
      - OPENAI_API_KEY=$LITELLM_MASTER_KEY
      
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:1416/status"]
      interval: 10s
      timeout: 5s
      retries: 18
      start_period: 5s

  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    ports:
      - "127.0.0.1:8080:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
      - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
      - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}

  letta_db:
    image: ankane/pgvector:v0.5.1
    networks:
      default:
        aliases:
          - pgvector_db
          - letta-db
    environment:
      - POSTGRES_USER=${LETTA_PG_USER:-letta}
      - POSTGRES_PASSWORD=${LETTA_PG_PASSWORD:-letta}
      - POSTGRES_DB=${LETTA_PG_DB:-letta}
    volumes:
      - ./.persist/pgdata-test:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"

  # Letta is an agent building framework with built-in memory/vectordb support.
  # https://docs.letta.com/quickstart/docker
  letta:
    build:
      context: .
      dockerfile: letta/Dockerfile
    container_name: letta
    ports:
      - 8283:8283
    volumes:
      # Mount the MCP configuration file (this points to Hayhooks)
      - ./letta_mcp_config.json:/root/.letta/mcp_config.json
    environment:
      LETTA_PG_DB: ${LETTA_PG_DB:-letta}
      LETTA_PG_USER: ${LETTA_PG_USER:-letta}
      LETTA_PG_PASSWORD: ${LETTA_PG_PASSWORD:-letta}
      LETTA_PG_HOST: pgvector_db
      LETTA_PG_PORT: 5432
      REQUESTS_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt
      # https://docs.letta.com/guides/server/providers/anthropic
      ANTHROPIC_API_KEY: $ANTHROPIC_API_KEY
      # https://docs.letta.com/guides/server/providers/google
      GEMINI_API_KEY: $GEMINI_API_KEY
      #OLLAMA_BASE_URL: $OLLAMA_BASE_URL
      # Do not attempt to use Ollama, LM Studio or OpenRouter chat models with Letta.
      # Letta is *very* picky and will only reliably work with the major cloud providers.
      # Embedding models will work fine, but only uncomment if you know what you're doing.
    restart: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8283/v1/health/"]
      interval: 5s
      timeout: 5s
      retries: 18
      start_period: 1s

  # Open WebUI is the front-end UI to Letta
  open-webui:
    image: ghcr.io/open-webui/open-webui:0.6.11
    container_name: open-webui
    volumes:
     - open-webui:/app/backend/data
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    environment:
      # https://docs.openwebui.com/getting-started/env-configuration/
      - GLOBAL_LOG_LEVEL=$OPENWEBUI_LOG_LEVEL
      # Disable admin login
      - WEBUI_AUTH=false
      # Enable the /docs endpoint for OpenAPI viewing
      #- ENV=dev
      # Prevent a langchain warning
      - USER_AGENT=openwebui
      #Â Set tags and titles explictly
      - ENABLE_TAGS_GENERATION=$ENABLE_TAGS_GENERATION
      - ENABLE_TITLE_GENERATION=$ENABLE_TITLE_GENERATION
      - TASK_MODEL=$TASK_MODEL
      - TASK_MODEL_EXTERNAL=$TASK_MODEL_EXTERNAL
      # Disable some meaningless options
      - ENABLE_EVALUATION_ARENA_MODELS=false
      - ENABLE_AUTOCOMPLETE_GENERATION=false
      - ENABLE_RETRIEVAL_QUERY_GENERATION=false
      # OpenAI selection should go to Hayhooks to show agents
      - ENABLE_OPENAI_API=true
      - OPENAI_API_BASE_URL=http://hayhooks:1416
      - OPENAI_API_KEY=no_key_required_for_hayhooks
      # Ollama Options
      - ENABLE_OLLAMA_API=$ENABLE_OLLAMA_API
      - OLLAMA_BASE_URL=$OLLAMA_BASE_URL
      # RAG options can be transformers, ollama, or openai 
      - RAG_EMBEDDING_ENGINE=$RAG_EMBEDDING_ENGINE
      - RAG_EMBEDDING_MODEL=$RAG_EMBEDDING_MODEL
      - RAG_OPENAI_API_BASE_URL=http://litellm:4000
      - RAG_OPENAI_API_KEY=$LITELLM_MASTER_KEY
      - RAG_OLLAMA_BASE_URL=$OLLAMA_BASE_URL
      # Tavily Web Search in Open WebUI
      - ENABLE_WEB_SEARCH=$ENABLE_WEB_SEARCH
      - WEB_SEARCH_ENGINE=$WEB_SEARCH_ENGINE
      - TAVILY_API_KEY=$TAVILY_API_KEY
      # Audio options
      - AUDIO_STT_ENGINE=$AUDIO_STT_ENGINE
    restart: unless-stopped
    # https://docs.openwebui.com/getting-started/advanced-topics/monitoring/#basic-health-check-endpoint
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 18
      start_period: 5s

  # This service runs after open-webui and letta has come up and is healthy
  # and adds an agent and tools to Letta and changes settings in Open WebUI.
  initializer:
    build: ./initializer
    container_name: initializer
    depends_on:
      open-webui:
        condition: service_healthy
      letta:
        condition: service_healthy
      hayhooks:
        condition: service_started
      searxng:
        condition: service_healthy
    environment:
      - OPEN_WEBUI_URL=http://open-webui:8080
      - LETTA_BASE_URL=http://letta:8283
      - HAYHOOKS_BASE_URL=http://hayhooks:1416
      - CHAT_MODEL=$LETTA_CHAT_MODEL
      - EMBEDDING_MODEL=$LETTA_EMBEDDING_MODEL

volumes:
  open-webui:
  caddy_data:
  caddy_config:
