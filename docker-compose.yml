services:

  litellm:
    build: litellm/
    container_name: litellm
    ports:
      - 4000:4000
    environment:
      - LITELLM_LOG=WARN
      - LITELLM_MASTER_KEY=$LITELLM_MASTER_KEY
      - GEMINI_API_KEY=$GEMINI_API_KEY
      - ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY
      # - OPENAI_API_KEY=$OPENAI_API_KEY
      
  # We only want Open WebUI talking to Hayhooks and Letta
  hayhooks:
    build: hayhooks/
    container_name: hayhooks
    ports:
      - 1416:1416
    depends_on:
      - litellm
    volumes:
      - ./hayhooks/pipelines:/pipelines
    environment:
      - OPENAI_API_BASE=http://litellm:4000
      - OPENAI_API_KEY=$LITELLM_MASTER_KEY
      - CHAT_MODEL=gemini-2.0-flash
      - HAYHOOKS_PIPELINES_DIR=/pipelines
      - HAYHOOKS_DISABLE_SSL=true
      - HAYHOOKS_HOST=0.0.0.0
      - HAYHOOKS_PORT=1416
      - LOG_LEVEL=INFO
      - HAYSTACK_LOG_LEVEL=INFO
      - HAYHOOKS_SHOW_TRACEBACKS=true
      # Needed for search pipeline
      - TAVILY_API_KEY=$TAVILY_API_KEY
      # Prevent __pycache__ directories
      - PYTHONDONTWRITEBYTECODE=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:1416/health"]
      interval: 10s
      timeout: 5s
      retries: 18
      start_period: 5s

  # https://docs.letta.com/quickstart/docker
  letta:
    image: letta/letta:0.6.48
    container_name: letta
    ports:
      - 8283:8283
    depends_on:
      - hayhooks
    volumes:
      # Mount the MCP configuration file (this points to Hayhooks)
      - ./letta_mcp_config.json:/root/.letta/mcp_config.json
    environment:
      # https://docs.letta.com/guides/server/docker#setting-environment-variables
      ANTHROPIC_API_KEY: $ANTHROPIC_API_KEY
      GEMINI_API_KEY: $GEMINI_API_KEY
    restart: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8283/v1/health/"]
      interval: 5s
      timeout: 5s
      retries: 18
      start_period: 1s

  open-webui:
    image: ghcr.io/open-webui/open-webui:0.6.0
    container_name: open-webui
    volumes:
     - open-webui:/app/backend/data
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    depends_on:
      - letta
      - hayhooks
    environment:
      # https://docs.openwebui.com/getting-started/env-configuration/
      - GLOBAL_LOG_LEVEL=WARNING
      # Disable admin login
      - WEBUI_AUTH=false
      # Enable the /docs endpoint for OpenAPI viewing
      - ENV=dev
      # Prevent a langchain warning
      - USER_AGENT=openwebui
      #Â Disable sneaky calls to LLMs by all methods possible
      - ENABLE_TAGS_GENERATION=false
      - ENABLE_TITLE_GENERATION=false
      - ENABLE_EVALUATION_ARENA_MODELS=false
      - ENABLE_AUTOCOMPLETE_GENERATION=false
      - ENABLE_RETRIEVAL_QUERY_GENERATION=false
      # Disable Ollama
      - ENABLE_OLLAMA_API=false
      # Point the RAG at Gemini to disable giant downloads
      - RAG_EMBEDDING_ENGINE=openai
      #- RAG_OPENAI_API_BASE_URL=http://litellm:4000
      #- RAG_OPENAI_API_KEY=$LITELLM_MASTER_KEY
      #- RAG_EMBEDDING_MODEL=text-embedding-004
      # Enable Tavily Web Search
      #- ENABLE_RAG_WEB_SEARCH=true
      #- RAG_WEB_SEARCH_ENGINE=tavily
      #- TAVILY_API_KEY=$TAVILY_API_KEY
      # Add models from LiteLLM
      #- OPENAI_API_BASE_URLS=http://litellm:4000
      #- OPENAI_API_KEY=$LITELLM_MASTER_KEY
      # Disable giant downloads
      - AUDIO_STT_ENGINE=openai
    restart: unless-stopped
    # https://docs.openwebui.com/getting-started/advanced-topics/monitoring/#basic-health-check-endpoint
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 18
      start_period: 5s

  # This service runs after open-webui and letta has come up and is healthy
  # and adds an agent and tools to Letta and changes settings in Open WebUI.
  initializer:
    build: ./initializer
    container_name: initializer
    depends_on:
      open-webui:
        condition: service_healthy
      letta:
        condition: service_healthy
    environment:
      - OPEN_WEBUI_URL=http://open-webui:8080
      - LETTA_BASE_URL=http://letta:8283
      # Gemini does not work reliably (will output in reasoning stage)
      #- LETTA_MODEL=google_ai/gemini-2.5-pro-exp-03-25
      # This is the default option if you don't have Gemini or Anthropic set up
      #- LETTA_MODEL=letta/letta-free
      # This is the preferred option
      - LETTA_MODEL=anthropic/claude-3-7-sonnet-20250219
      

volumes:
  open-webui:
